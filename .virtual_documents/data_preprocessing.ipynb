# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path


# Load the dataset
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# Set display options for better visualization
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)

# Define data path
data_dir = Path("Data")
fake_data_1_path = data_dir / "fake_data_1.csv"
fake_data_2_path = data_dir / "fake_data_2.csv"
fake_data_3_path = data_dir / "fake_data_3.csv"
fake_data_4_path = data_dir / "fake_data_4.csv"

real_data_1_path = data_dir / "real_data_1.csv"
real_data_2_path = data_dir / "real_data_2.csv"
real_data_3_path = data_dir / "real_data_3.csv"
real_data_4_path = data_dir / "real_data_4.csv"

columns = ["title", "text", "subject", "date", "real"]

# Read the CSV files
df_train_1 = pd.read_csv(fake_data_1_path, header=None, names=columns)
df_train_2 = pd.read_csv(fake_data_2_path, header=None, names=columns)
df_train_3 = pd.read_csv(fake_data_3_path, header=None, names=columns)
df_train_4 = pd.read_csv(fake_data_4_path, header=None, names=columns)

df_train_5 = pd.read_csv(real_data_1_path, header=None, names=columns)
df_train_6 = pd.read_csv(real_data_2_path, header=None, names=columns)
df_train_7 = pd.read_csv(real_data_3_path, header=None, names=columns)
df_train_8 = pd.read_csv(real_data_4_path, header=None, names=columns)

# Combine datasets into one
df = pd.concat([df_train_1, df_train_2, df_train_3, df_train_4, df_train_4, df_train_5, df_train_6, df_train_7, df_train_8], ignore_index=True)
df = df[df['title'] != "title"]

# Explore data structure
print("Dataset Shape:", df.shape)
df.head(10)


# Eliminate duplicate articles
df = df.drop_duplicates(subset=['text'], keep='first')  
print("Dataset Shape:", df.shape)
df.head(10)


# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())


# Drop rows with missing statements (as statement is critical for NLP analysis)
df.dropna(subset=["text"], inplace=True)


# Fill missing categorical values with 'Unknown'
categorical_columns = ["title", "text", "subject"]
for col in categorical_columns:
    df[col].fillna("Unknown", inplace=True)


# Drop unnecessary columns (id is not useful for ML)
#df.drop(columns=["title"], inplace=True)

# Remove duplicates
df.drop_duplicates(inplace=True)

# Summary of the cleaned dataset
print("\nCleaned Dataset Shape:", df.shape)
df.head()


# Save cleaned dataset
df.to_csv("data_cleaned.csv", index=False)
print("\nCleaned data saved as data_cleaned.csv")


# Quick visualization of label distribution
plt.figure(figsize=(6, 4))
sns.countplot(x=df['real'])
plt.title("Distribution of Fake vs Real News")
plt.xticks(ticks=[0, 1], labels=["Fake", "Real"])
plt.xlabel("Real")
plt.ylabel("Count")
plt.show()



